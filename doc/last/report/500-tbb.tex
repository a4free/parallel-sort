\section{Threading Building Blocks}
\label{sec:500}

Intel\tr developed the C++ Template Library Threading Building Blocks to offer a parallelization approach where the focus of the programmer could stay away from the parallelism intrinsic issues, and stay on the algorithm and \textit{useful} code.

The library attempts to provide an abstraction layer for the parallelism, effectively providing an almost completely different paradigm than the OpenMP library.

The most basic parallel operator in TBB is the \texttt{parallel\_for} function, which is the equivalent of \texttt{\#pragma omp parallel for} directive in OpenMP. This function takes as an argument a \textit{Functor}, or a class instance that defines the computation to be done in parallel.

TBB deals with the thread allocation and initialization, and also manages the workload, by dividing the range of iteration in chuncks. Each thread will receive a chunck that can be iterated just like a regular C++ collection. The \textit{Functor} allocated on that thread can then iterate through the chunck and perform the desired function, with full transparency from the threading and work assigning logic.

Besides the particular implementation details, the Radix Sort implementation follows a similar structure to the OpenMP implementation. Since Radix Sort consists mostly on array operations and iterations, there was no need for any TBB more complex operators like \texttt{parallel\_reduce} or \texttt{parallel\_pipeline}. However, the \texttt{concurrent\_vector} class proved useful for some specific issues that required the usage of explicit locks in the OpenMP version.

\subsection{TBB Paradigm}
It can be said that TBB, while providing excellent facilities to parallelize tasks and organize workload, also suffers from a couple of problems due to the paradigm it attempts to implement. Drawing much from functional paradigms, it requires the parallelizable logic to be encapsulated in a class object (namely, a \textit{Functor}). While this is good from a more abstract point of view, it also requires much more code to be written, for tasks that are sometimes rather simple. This issue might be solved with the usage of C++11 (recenlty known as C++0x), which allows the declaration of lambda functions. TBB supports this functions, allowing them to be passed as parameter to the parallelization directives, instead of an instance of a \textit{Functor}.

Another point that might provide some pitfalls to TBB is the complete abstraction of the threading model. While it is usually good to let the library handle all the background tasks of allocating threads and distribute the workload, this is not always the case. For most tasks it also possible to parameterize TBB functions, for example, with information about the desired granularity, which is useful to have some degree of control about the degree of division of the task.

It is not possible however, to have more low-level control over this. A particular example was found during the implementation of Radix Sort. Having each thread handling a set of buckets, with the index of those buckets being received as the chunck list to each thread, it was required to decide, for each element, wether or not the current thread was the one responsible for saving it. In OpenMP, this simply meant acquiring the thread id with the appropriate number, and decide from there if the destiny bucket was being handled by the current thread.
In TBB, however, there is no such thing as a thread id. This makes sense, but only to a certain extent, since it is TBB who decides how many threads are spawned. The only possible control over this is the maximum number of threads, but never the actual exact amount can be specified.
Because of this, the only way to determine if the bucket was assigned to the current thread was, for each element to insert, iterate over the entire chunck list, and check wether or not the index was there. This changed this computation from a simple bitwise operation and a comparison in OpenMP, to an entire loop in TBB.

Even though the chunck is probably small enough to eliminate any performance issues from this difference, the fact remains that the lack of a more controlable parallelization feature hurt this version.